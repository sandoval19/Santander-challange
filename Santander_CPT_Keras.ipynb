{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Santander_CPT_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandoval19/Santander-challange/blob/master/Santander_CPT_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKWN40kqT5lM",
        "colab_type": "text"
      },
      "source": [
        "Import the packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxDC_u5CGDea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToshPbvdUDLn",
        "colab_type": "text"
      },
      "source": [
        "Check if everything is okay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah2nnjhPDoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_correct_tf_version = '1.14.' in tf.__version__\n",
        "assert is_correct_tf_version, \"Wrong tensorflow version {} installed\".format(tf.__version__)\n",
        "\n",
        "is_eager_enabled = tf.executing_eagerly()\n",
        "assert is_eager_enabled,      \"Tensorflow eager mode is not enabled\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta3p9e8Jk6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKFcEFGVmHi",
        "colab_type": "text"
      },
      "source": [
        "#Análisis de los datos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axutW8s_Vkxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "print(\"dimensiones del train set:{}, test set:{}\".format(train_data.shape,test_data.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk42QjUHT31w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missing_data(data):\n",
        "\t\"\"\"\n",
        "\tAnalyze the data, which its type is dataFrame to look for missing total values\n",
        "\tinput: dataFrame \n",
        "\toutput: if not: Boolean, if there is missing data: dataFrame with missing values per colums\n",
        "\t\"\"\"\n",
        "\tflag = data.isna().sum().any()\n",
        "\tif flag:\n",
        "\t\tnull_data=data.isnull().sum()\n",
        "\t\tpercent= (null_data/data.isnull().count())*100\n",
        "\n",
        "\t\t#new data frame to return \n",
        "\t\tmis_data= pd.concat([null_data,percent],axis=1,keys=['Total','Percent'])\n",
        "\t\tcolumns_type=[]\n",
        "\t\tfor col in data.columns:\n",
        "\t\t\tdtype= str(data[col].dtype)\n",
        "\t\t\tcolumns_type.append(dtype)\n",
        "\t\tmis_data['Type'] = columns_type\n",
        "\t\treturn (np.transpose(mis_data))\n",
        "\telse:\n",
        "\t\treturn(False)\n",
        "\n",
        "df_count_train=(missing_data(train_data))\n",
        "print(df_count_train)\n",
        "df_count_test=(missing_data(test_data))\n",
        "print(df_count_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQV3T9w9VzXa",
        "colab_type": "text"
      },
      "source": [
        "Se puede observar para cada feature no hay valores perdidos\n",
        "\n",
        "Ahora se analizan algunos valores estadisticos de los data set: total, mean, std, min, percentils."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5ZLdnPV5LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train_data.describe()\n",
        "print(train_stats)\n",
        "test_stats = test_data.describe()\n",
        "print(test_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfMi6kqyWDj4",
        "colab_type": "text"
      },
      "source": [
        "**Al ver la desviación estander y mean se puede inferir que los features (columnas) no estan normalizados: diferentes std y mean para cada feature.**\n",
        "\n",
        "**Tampoco escalados: rangos de min-max considerables para cada feature.\n",
        "Se considera hacer escalamiento, normalización/estandarización**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4fm6gSDWxYi",
        "colab_type": "text"
      },
      "source": [
        "Se Analiza la distribución de datos para ambos casos target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gUKslKYWqnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(train_data['target'],palette='Set2')\n",
        "plt.show()\n",
        "\n",
        "#pandas acomoda de manera descendente la predominancia\n",
        "print( train_data[\"target\"].value_counts())\n",
        "print(\"There are {}% target values with 1\".format(100 * train_data[\"target\"].value_counts()[1]/train_data.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nwjvO8TW8-K",
        "colab_type": "text"
      },
      "source": [
        "Se muestrea el set de train para generar un data set mas parejo en clases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq9syuulXc8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ones = train_data[train_data['target'] > 0]\n",
        "print(\"Shape of Clase One's\",df_ones.shape)\n",
        "#se toman un 15% de la clase 1 \n",
        "fracc=0.15\n",
        "df_zeros = train_data[train_data['target'] == 0].sample(frac=fracc)\n",
        "print(\"Shape {} of Clase Zero's with: {} '%' of the original set\".format(df_zeros.shape,fracc))\n",
        "#we concat both to the sampling dataframe\n",
        "#if frac is used with value 1 it will return all the data but shuffled\n",
        "train_data = pd.concat([df_ones, df_zeros]).sample(frac=1) #shuffling\n",
        "test_data= test_data.sample(n=train_data.shape[0])\n",
        "print(\"Shape of the new data sampled:\",train_data.shape)\n",
        "print(\"Shape of the new test_data: \", test_data.shape)\n",
        "print(train_data.head())\n",
        "print(train_data.tail())\n",
        "\n",
        "sns.countplot(train_data['target'],palette='Set2')\n",
        "plt.title('New data distribution after regroup')\n",
        "plt.show()\n",
        "print( train_data[\"target\"].value_counts())\n",
        "\n",
        "print(\"There are {}% target values with 1\".format(100 * train_data[\"target\"].value_counts()[1]/train_data.shape[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk_yo0AKYIW3",
        "colab_type": "text"
      },
      "source": [
        "Se analiza la distribución normal de las features(columns) para los dos casos de target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtJPeqLhYJUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_feature_distribution(df1, df2, label1, label2, features):\n",
        "    \"\"\"\n",
        "    Grafica la probabilidad de la funcion de densidad (PDF) para ambos dataframes \n",
        "    basado en que las features estan en ambos dataframes y las etiqueta dependiendo del label\n",
        "    input: dataframe1, dataframe2, string:label 1, string:label 2,lista/serie:features\n",
        "    output: Nan\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    sns.set_style('whitegrid')\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(10,10,figsize=(22,22))\n",
        "    for feature in features:\n",
        "        i += 1\n",
        "        plt.subplot(10,10,i)\n",
        "        sns.distplot(df1[feature], hist=False,label=label1)\n",
        "        sns.distplot(df2[feature], hist=False,label=label2)\n",
        "        plt.xlabel(feature, fontsize=9)\n",
        "        locs, labels = plt.xticks()\n",
        "        plt.tick_params(axis='x', which='major', labelsize=6)\n",
        "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztydO_AtbDvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#primeros 100 features\n",
        "t0 = train_data.loc[train_data['target'] == 0]\n",
        "t1 = train_data.loc[train_data['target'] == 1]\n",
        "features1 = train_data.columns.values[2:102]\n",
        "print(len(features1))\n",
        "plot_feature_distribution(t0, t1, '0', '1', features1)\n",
        "#los 100 features restantes\n",
        "features2 = train_data.columns.values[102:202]\n",
        "plot_feature_distribution(t0, t1, '0', '1', features2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpJTDaYRbNFN",
        "colab_type": "text"
      },
      "source": [
        "**Se analizar si entre los dos target alguna feature se diferencia mucho entre 1 y 0,\n",
        "puede ser un factor importante que el modelo aprenda a diferenciar entre las dos clases.\n",
        "Así también como features que sean muy similares, brindarían poca información para diferencias las clases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV5ewb3SbwUT",
        "colab_type": "text"
      },
      "source": [
        "Ahora se analiza la distribución de las features(columns) para los set de train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9JJsdTVbHU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_feature_distribution(train_data,test_data,'train','test',features1)\n",
        "plot_feature_distribution(train_data,test_data,'train','test',features2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9eVMMuHb0ax",
        "colab_type": "text"
      },
      "source": [
        "**Se puede analizar que entre train y test algunas features o son similares en distribución,\n",
        "o son muy distintas.\n",
        "Esto puede llevar a un criterio de selccion de features, ya que al ser distintas en train y test\n",
        "el modelo aprendera un comportamiento para esa feature que nunca va a ver en el test, así clasificando \n",
        "de manera erronea.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl_q-ImsYNVO",
        "colab_type": "text"
      },
      "source": [
        "Analizar si los ejemplos(rows) en test set mantienen una distribución de media similar a el train set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxiojc0cgFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_tot = train_data.columns.values[2:202]\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.title(\"Distribution of mean values per row in the train and test set\")\n",
        "sns.distplot(train_data[features_tot].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\n",
        "sns.distplot(test_data[features_tot].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3z-tKEicxrN",
        "colab_type": "text"
      },
      "source": [
        "**Se puede analizar para determinar que los ejemplos/datos del test permiten de manera adecuada\n",
        "evaluar el modelo una vez aprendido del train set\n",
        "Con ejemplos atipicos puede suceder que el modelo lo clasifique de manera erronea. Se observa siguen una distrinución muy similar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVA8j4X1dDWY",
        "colab_type": "text"
      },
      "source": [
        "Analizar la distribución de valores por feature para las clases 0 y 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1rmszwHc0zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = train_data.loc[train_data['target'] == 0]\n",
        "t1 = train_data.loc[train_data['target'] == 1]\n",
        "\n",
        "def plot_max_min_colum(dtf1,dtf2,label1,label2,features):\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.title(\"Distribution of min values per column in the train set\")\n",
        "    sns.distplot(dtf1[features].min(axis=0),color=\"orange\", kde=True,bins=120, label='target = '+str(label1))\n",
        "    sns.distplot(dtf2[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = '+str(label2))\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.title(\"Distribution of max values per column in the train set\")\n",
        "    sns.distplot(dtf1[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = '+str(label1))\n",
        "    sns.distplot(dtf2[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = '+str(label2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_max_min_colum(t0,t1,'1','0',features_tot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjegxjWweNz5",
        "colab_type": "text"
      },
      "source": [
        "**Se re afirma la idea de normalizar los datos, debido a la grande distancia entre valores minimos y maximos features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7qM-QwecFi",
        "colab_type": "text"
      },
      "source": [
        "Elimina la muestra repetida y dejar solo la primera. Esto con el fin de evitar un bias a una muestra en particular. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hoBkj0EePQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=train_data.drop_duplicates(keep='first')\n",
        "print(train_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coDDqbfBe0UM",
        "colab_type": "text"
      },
      "source": [
        "**Se puede observar que el dataset no tiene muestras repetidas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsU2-BsolEvQ",
        "colab_type": "text"
      },
      "source": [
        "Se normaliza el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8fjfV7Nk9jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "    \"\"\"\n",
        "    Normalize the data\n",
        "    input dataframe\n",
        "    output dataframe normalized\n",
        "    \"\"\"\n",
        "    #Following the formula \n",
        "    # z =  x - U / g \n",
        "    stats=x.describe()\n",
        "    stats=stats.transpose()\n",
        "    return (x - stats['mean'])/stats['std']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyUsIyg7lURW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=norm(train_data)\n",
        "test_data=norm(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpyC0Zzglf8D",
        "colab_type": "text"
      },
      "source": [
        "#Featuring engineering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIJ1cndjrLSe",
        "colab_type": "text"
      },
      "source": [
        " Una vez analizados los set de datos, se propone agregar variables que permitan capturar información para los target 1 y 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hroHhAbJs6gC",
        "colab_type": "text"
      },
      "source": [
        "Primero se dividen los datos para entrenar X y su correspondiente  label Y. Además se genera el vector para test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sUJTre_rKmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_data.iloc[:,2:]\n",
        "Y=train_data['target']\n",
        "x_test=test_data.iloc[:,1:]\n",
        "print(\"x_train\")\n",
        "print(X.head())\n",
        "print(\"Shape new X train:\", X.shape)\n",
        "print(\"y_train\")\n",
        "print(Y.head())\n",
        "print(Y.shape)\n",
        "print(\"test_data\")\n",
        "print(x_test.head())\n",
        "print(\"Shape new X test:\",x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnPmuuvktHTl",
        "colab_type": "text"
      },
      "source": [
        "Se agregan columnas de valores estadisticos calculados a apartir de cada muestra(filas) como: la suma total, la media, std, max y min, y el valor medio. Esto considerando que es posible extaer valor que ayude a clasificar los target 0 y 1. Para ambos train y test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tIYaUKAlXOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = train_data.columns.values[2:202]\n",
        "for df in [X, x_test]:\n",
        "    df['sum'] = df[idx].sum(axis=1)  \n",
        "    df['min'] = df[idx].min(axis=1)\n",
        "    df['max'] = df[idx].max(axis=1)\n",
        "    df['mean'] = df[idx].mean(axis=1)\n",
        "    df['std'] = df[idx].std(axis=1)\n",
        "    df['med'] = df[idx].median(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DQRG0UBuFAV",
        "colab_type": "text"
      },
      "source": [
        "Ahora aplicamos estandarización a los datos por columna. Dejandolos con media 0 y std 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw-o7fNKuBOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "    \"\"\"\n",
        "    Normalize the data\n",
        "    input dataframe\n",
        "    output dataframe normalized\n",
        "    \"\"\"\n",
        "    #Following the formula \n",
        "    # z =  x - U / g \n",
        "    stats=x.describe()\n",
        "    stats=stats.transpose()\n",
        "    return (x - stats['mean'])/stats['std']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDVaCeDuVt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=norm(X)\n",
        "x_test=norm(x_test)\n",
        "#X.to_pickle(\"x_train_normalized.pkl\")\n",
        "#Y.to_pickle(\"y_train_normalized.pkl\")\n",
        "#x_test.to_pickle(\"x_test_normalized.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFXloILmvP6w",
        "colab_type": "text"
      },
      "source": [
        "#Modelo DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtvjLJnvULV",
        "colab_type": "text"
      },
      "source": [
        "Una vez al tener el dataset necesario se genera el modelo con el cual se clasificará el usuario bajo la etiqueta de 0(realiza la transaacción) y 1(no realizá la transacción)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJ75n3ovu0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train=pd.read_pickle('x_train_normalized.pkl')\n",
        "#y_train=pd.read_pickle('y_train_normalized.pkl')\n",
        "#x_predic=pd.read_pickle('x_test_normalized.pkl')\n",
        "tst_size=0.3\n",
        "x_train=X.copy()\n",
        "y_train=Y.copy()\n",
        "x_predic=x_test.copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDlxkJP0KLr",
        "colab_type": "text"
      },
      "source": [
        "Se dividio en dataset en entrenamiento y prueba. Posteriormente se divide el entrenamiento en una fracción de 0.2 para validación cruzada.\n",
        "Quedando finalmente:\n",
        "\n",
        "\n",
        "> entrenamiento pasa de 32958 a 26366\n",
        "\n",
        "> validación cruzada  6592\n",
        "\n",
        "> prueba 14125\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBxjqlov0JiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test train split\n",
        "\n",
        "X_train, X_test, Y_train, y_test = train_test_split(x_train, y_train, test_size=tst_size, random_state=6666)\n",
        "print(\"X_train: \", X_train.shape)\n",
        "#print((X_train.head()))\n",
        "print(\"X_test: \" ,X_test.shape)\n",
        "#print(X_test.head())\n",
        "print(\"Y_train: \",Y_train.shape)\n",
        "#print((Y_train.head()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnJmHc-1EgN",
        "colab_type": "text"
      },
      "source": [
        "Se crean tres modelos diferentes, variando la funcion de optimización, el numero de capas ocultas y las funciones de activación según criterio de diseño. Adicionalmente para el primer modelo se utilizan regulaziración por L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPZfCwm61AcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buil_model(option=1,layer_size=100,hidden_units=3):\n",
        "\t\"\"\"\n",
        "\tcreate a DNN model\n",
        "\tinput option to select different models, # hidden neurons,  # hidenn units\n",
        "\touput DNN model \n",
        "\t\"\"\"\n",
        "\tif option == 1:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(200,input_shape=(X_train.shape[1],),kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\t#model.add(layers.Dropout(0.3))\t\t\n",
        "\t\t\n",
        "\t\tmodel.add(layers.Dense(200,kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\t\n",
        "\n",
        "\t\tmodel.add(layers.Dense(50,kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tmodel.add(layers.Dense(1,activation='sigmoid'))\n",
        "\t\tadam=keras.optimizers.Adam(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc'])\n",
        "\t\treturn model\n",
        "\tif option == 2:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(layer_size,input_shape=(x_train.shape[1],)))\n",
        "\t\tmodel.add(layers.Activation('sigmoid'))\n",
        "\t\t#model.add(layers.Dropout(0.3))\n",
        "\t\tfor i in range(hidden_units):\n",
        "\t\t\tmodel.add(layers.Dense(layer_size))\n",
        "\t\t\tmodel.add(layers.Activation('sigmoid'))\n",
        "\t\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\tmodel.add(layers.Dense(1,activation='relu'))\n",
        "\t\tadam=keras.optimizers.RMSprop(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc'])\n",
        "\t\treturn model\n",
        "\n",
        "\tif option == 3:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(layer_size,input_shape=(x_train.shape[1],)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\t#model.add(layers.Dropout(0.3))\n",
        "\t\tfor i in range(hidden_units):\n",
        "\t\t\tmodel.add(layers.Dense(layer_size))\n",
        "\t\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\tmodel.add(layers.Dense(1,activation='sigmoid'))\n",
        "\t\tadam=keras.optimizers.SGD(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc'])\n",
        "\t\treturn model\n",
        "\telse:\n",
        "\t\traise ValueError('func called with bad args')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOfkfs6m1VbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1=buil_model(1)\n",
        "model2=buil_model(2)\n",
        "model3=buil_model(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9pZ03I51o6D",
        "colab_type": "text"
      },
      "source": [
        "Se utiliza Earlystoping con el objeto de detener el entrenamiento si el criterio de validación cruzada no muestra mejoría pasadas 15 epocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84zsL5xU1oJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qr5kev2BVq",
        "colab_type": "text"
      },
      "source": [
        "Se entrenan los modelos utilizando batch de tamaño 32 y por 100 epocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi6JOsrD17RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model1.summary())\n",
        "history1=model1.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2, \n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_8taH4a18_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model2.summary())\n",
        "history2=model2.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2,\n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdlx6Ec1_1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model3.summary())\n",
        "history3=model3.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2,\n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttI1bKxB2SP6",
        "colab_type": "text"
      },
      "source": [
        "Una vez entrenados los modelos se compara las curvas de aprendizaje "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODJbOXZj2RmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history1,history2,history3):\n",
        "\t\"\"\"\n",
        "\tPlot the acc,los for both validation and train for each model.\n",
        "\t\"\"\"\n",
        "\n",
        "\thist1 = pd.DataFrame(history1.history)\n",
        "\thist1['epoch'] = history1.epoch\n",
        "\thist2 = pd.DataFrame(history2.history)\n",
        "\thist2['epoch'] = history2.epoch\n",
        "\thist3 = pd.DataFrame(history3.history)\n",
        "\thist3['epoch'] = history3.epoch\n",
        "\n",
        "\n",
        "\t#print(\"\\n\",hist1.head())\n",
        "\t#print(\"\\n\",hist1.tail())\n",
        "\tacc=history1.history['acc']\n",
        "\tval_acc=history1.history['val_acc']\n",
        "\tloss=history1.history['loss']\n",
        "\tval_loss=history1.history['val_loss']\n",
        "\n",
        "\tacc2=history2.history['acc']\n",
        "\tval_acc2=history2.history['val_acc']\n",
        "\tloss2=history2.history['loss']\n",
        "\tval_loss2=history2.history['val_loss']\n",
        "\n",
        "\tacc3=history3.history['acc']\n",
        "\tval_acc3=history3.history['val_acc']\n",
        "\tloss3=history3.history['loss']\n",
        "\tval_loss3=history3.history['val_loss']\n",
        "\n",
        "\n",
        "\n",
        "\tplt.figure(figsize=(8,8))\n",
        "\tplt.subplot(3,2,1)\n",
        "\tplt.plot(hist1['epoch'],acc,label='Training Accuracy')\n",
        "\tplt.plot(hist1['epoch'],val_acc,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model1')\n",
        "\n",
        "\tplt.subplot(3,2,2)\n",
        "\tplt.plot(hist1['epoch'],loss,label='Training Loss')\n",
        "\tplt.plot(hist1['epoch'],val_loss,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model1')\n",
        "\n",
        "\tplt.subplot(3,2,3)\n",
        "\tplt.plot(hist2['epoch'],acc2,label='Training Accuracy')\n",
        "\tplt.plot(hist2['epoch'],val_acc2,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model2')\n",
        "\n",
        "\tplt.subplot(3,2,4)\n",
        "\tplt.plot(hist2['epoch'],loss2,label='Training Loss')\n",
        "\tplt.plot(hist2['epoch'],val_loss2,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model2')\n",
        "\n",
        "\tplt.subplot(3,2,5)\n",
        "\tplt.plot(hist3['epoch'],acc3,label='Training Accuracy')\n",
        "\tplt.plot(hist3['epoch'],val_acc3,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model3')\n",
        "\n",
        "\tplt.subplot(3,2,6)\n",
        "\tplt.plot(hist3['epoch'],loss3,label='Training Loss')\n",
        "\tplt.plot(hist3['epoch'],val_loss3,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model3')\n",
        "\n",
        "\tplt.show()\n",
        "\n",
        "plot_history(history1,history2,history3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPTNTosu24Hy",
        "colab_type": "text"
      },
      "source": [
        "Se evaluan los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b57d73o42ak1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model evualte \n",
        "test_lost1,test_accuracy1 = model1.evaluate(X_test,y_test,verbose=1)\n",
        "test_lost2,test_accuracy2 = model2.evaluate(X_test,y_test,verbose=1)\n",
        "test_lost3,test_accuracy3 = model3.evaluate(X_test,y_test,verbose=1)\n",
        "print('model 1(Adam) test loss: {} accuracy test: {} '.format(test_lost1,test_accuracy1))\n",
        "print('model 2(RMSprop) test loss: {} accuracy test: {} '.format(test_lost2,test_accuracy2))\n",
        "print('model 3(SGD) test loss: {} accuracy test: {} '.format(test_lost3,test_accuracy3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdy7ZaFv36oe",
        "colab_type": "text"
      },
      "source": [
        "Se predice usando los modelos entrenados y datos sin etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNFAdrRx31l_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_pred_nn = model1.predict(x_predic)[:,0]\n",
        "target_pred_nn2 = model2.predict(x_predic)[:,0]\n",
        "target_pred_nn3 = model3.predict(x_predic)[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzRL5b-E3vsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13, 9))\n",
        "sns.distplot(target_pred_nn,label='NN model 1(Adam) Target')\n",
        "sns.distplot(target_pred_nn2,label='NN model 2(RMSprop) Target')\n",
        "sns.distplot(target_pred_nn3,label='NN model 3(SGD) Target')\n",
        "plt.title('Test set target predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}