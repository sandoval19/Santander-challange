{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Santander_CPT_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandoval19/Santander-challange/blob/master/Santander_CPT_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKWN40kqT5lM",
        "colab_type": "text"
      },
      "source": [
        "Import the packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxDC_u5CGDea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToshPbvdUDLn",
        "colab_type": "text"
      },
      "source": [
        "Check if everything is okay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah2nnjhPDoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_correct_tf_version = '1.14.' in tf.__version__\n",
        "assert is_correct_tf_version, \"Wrong tensorflow version {} installed\".format(tf.__version__)\n",
        "\n",
        "is_eager_enabled = tf.executing_eagerly()\n",
        "assert is_eager_enabled,      \"Tensorflow eager mode is not enabled\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta3p9e8Jk6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKFcEFGVmHi",
        "colab_type": "text"
      },
      "source": [
        "#An√°lisis de los datos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tDuXoQ5GJYE",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "40e142b0-8ca6-42ca-d8ca-fd3339eb35b1"
      },
      "source": [
        "#Upload data from local \n",
        "from google.colab import files\n",
        "train_data = files.upload()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fbe1fd6f-b9dc-4fcf-9f27-a814407faf92\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fbe1fd6f-b9dc-4fcf-9f27-a814407faf92\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MBcHk84G1y4",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "5f7cc4fb-d67d-4122-bf27-09ec297d705a"
      },
      "source": [
        "test_data = files.upload()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff954605-f64e-408c-b471-420284f86eaa\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ff954605-f64e-408c-b471-420284f86eaa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axutW8s_Vkxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "110b49ac-f9db-4896-ae47-ec6af557b5dc"
      },
      "source": [
        "import io \n",
        "train_data=pd.read_csv(io.BytesIO(uploaded_A['training_normal_A.csv'])\n",
        "\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "print(\"dimensiones del train set:{}, test set:{}\".format(train_data.shape,test_data.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-04b593c920a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dimensiones del train set:{}, test set:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk42QjUHT31w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missing_data(data):\n",
        "\t\"\"\"\n",
        "\tAnalyze the data, which its type is dataFrame to look for missing total values\n",
        "\tinput: dataFrame \n",
        "\toutput: if not: Boolean, if there is missing data: dataFrame with missing values per colums\n",
        "\t\"\"\"\n",
        "\tflag = data.isna().sum().any()\n",
        "\tif flag:\n",
        "\t\tnull_data=data.isnull().sum()\n",
        "\t\tpercent= (null_data/data.isnull().count())*100\n",
        "\n",
        "\t\t#new data frame to return \n",
        "\t\tmis_data= pd.concat([null_data,percent],axis=1,keys=['Total','Percent'])\n",
        "\t\tcolumns_type=[]\n",
        "\t\tfor col in data.columns:\n",
        "\t\t\tdtype= str(data[col].dtype)\n",
        "\t\t\tcolumns_type.append(dtype)\n",
        "\t\tmis_data['Type'] = columns_type\n",
        "\t\treturn (np.transpose(mis_data))\n",
        "\telse:\n",
        "\t\treturn(False)\n",
        "\n",
        "df_count_train=(missing_data(train_data))\n",
        "print(df_count_train)\n",
        "df_count_test=(missing_data(test_data))\n",
        "print(df_count_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQV3T9w9VzXa",
        "colab_type": "text"
      },
      "source": [
        "The data set doesn't have missing values\n",
        "\n",
        "Now we describe some statistical values from train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5ZLdnPV5LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train_data.describe()\n",
        "print(train_stats)\n",
        "test_stats = test_data.describe()\n",
        "print(test_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfMi6kqyWDj4",
        "colab_type": "text"
      },
      "source": [
        "**Analizing the std and mean values it is possible to conclude that the features are not normalized.**\n",
        "\n",
        "**Also it is not scaled due to the Min-Max values gap.\n",
        "It is possible to appply both scale and normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4fm6gSDWxYi",
        "colab_type": "text"
      },
      "source": [
        "Display the number of examples per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gUKslKYWqnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(train_data['target'],palette='Set2')\n",
        "plt.show()\n",
        "\n",
        "#pandas acomoda de manera descendente la predominancia\n",
        "print( train_data[\"target\"].value_counts())\n",
        "print(\"There are {}% target values with 1\".format(100 * train_data[\"target\"].value_counts()[1]/train_data.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nwjvO8TW8-K",
        "colab_type": "text"
      },
      "source": [
        "Because of the data is unbalanced, it is necessary to sample the data in order to obtain a better balance btw the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq9syuulXc8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ones = train_data[train_data['target'] > 0]\n",
        "print(\"Shape of Clase One's\",df_ones.shape)\n",
        "#se toman un 15% de la clase 1 \n",
        "fracc=0.15\n",
        "df_zeros = train_data[train_data['target'] == 0].sample(frac=fracc)\n",
        "print(\"Shape {} of Clase Zero's with: {} '%' of the original set\".format(df_zeros.shape,fracc))\n",
        "#we concat both to the sampling dataframe\n",
        "#if frac is used with value 1 it will return all the data but shuffled\n",
        "train_data = pd.concat([df_ones, df_zeros]).sample(frac=1) #shuffling\n",
        "test_data= test_data.sample(n=train_data.shape[0])\n",
        "print(\"Shape of the new data sampled:\",train_data.shape)\n",
        "print(\"Shape of the new test_data: \", test_data.shape)\n",
        "print(train_data.head())\n",
        "print(train_data.tail())\n",
        "\n",
        "sns.countplot(train_data['target'],palette='Set2')\n",
        "plt.title('New data distribution after regroup')\n",
        "plt.show()\n",
        "print( train_data[\"target\"].value_counts())\n",
        "\n",
        "print(\"There are {}% target values with 1\".format(100 * train_data[\"target\"].value_counts()[1]/train_data.shape[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk_yo0AKYIW3",
        "colab_type": "text"
      },
      "source": [
        "Here we look at the normal distribution of the features for both classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtJPeqLhYJUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_feature_distribution(df1, df2, label1, label2, features):\n",
        "    \"\"\"\n",
        "    Grafica la probabilidad de la funcion de densidad (PDF) para ambos dataframes \n",
        "    basado en que las features estan en ambos dataframes y las etiqueta dependiendo del label\n",
        "    input: dataframe1, dataframe2, string:label 1, string:label 2,lista/serie:features\n",
        "    output: Nan\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    sns.set_style('whitegrid')\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(10,10,figsize=(22,22))\n",
        "    for feature in features:\n",
        "        i += 1\n",
        "        plt.subplot(10,10,i)\n",
        "        sns.distplot(df1[feature], hist=False,label=label1)\n",
        "        sns.distplot(df2[feature], hist=False,label=label2)\n",
        "        plt.xlabel(feature, fontsize=9)\n",
        "        locs, labels = plt.xticks()\n",
        "        plt.tick_params(axis='x', which='major', labelsize=6)\n",
        "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztydO_AtbDvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#primeros 100 features\n",
        "t0 = train_data.loc[train_data['target'] == 0]\n",
        "t1 = train_data.loc[train_data['target'] == 1]\n",
        "features1 = train_data.columns.values[2:102]\n",
        "print(len(features1))\n",
        "plot_feature_distribution(t0, t1, '0', '1', features1)\n",
        "#los 100 features restantes\n",
        "features2 = train_data.columns.values[102:202]\n",
        "plot_feature_distribution(t0, t1, '0', '1', features2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpJTDaYRbNFN",
        "colab_type": "text"
      },
      "source": [
        "**The figure above helps to analize the correlation btw the features for both classes. Ex. if a PDF's feature is too much different in class 1 compared to 0, this feature will contribute to difference btw the classes.\n",
        "Also if a PDF's feature is similar in both classes it probably will not contribute too much.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV5ewb3SbwUT",
        "colab_type": "text"
      },
      "source": [
        "Here we look at the normal distribution of the features for train set and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9JJsdTVbHU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_feature_distribution(train_data,test_data,'train','test',features1)\n",
        "plot_feature_distribution(train_data,test_data,'train','test',features2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9eVMMuHb0ax",
        "colab_type": "text"
      },
      "source": [
        "**The differences btw the features for both train and test are a criteria for feature selection.\n",
        "Ex. if the features are different, the model maybe fall in miss classify that one because the model learned a different behavior for that feature.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl_q-ImsYNVO",
        "colab_type": "text"
      },
      "source": [
        "Here we look at the normal distribution of the features per example in both test and train sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxiojc0cgFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_tot = train_data.columns.values[2:202]\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.title(\"Distribution of mean values per row in the train and test set\")\n",
        "sns.distplot(train_data[features_tot].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\n",
        "sns.distplot(test_data[features_tot].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3z-tKEicxrN",
        "colab_type": "text"
      },
      "source": [
        "**It is possible to conclude that the examples remain a similar  distribution in train and test. So  those can be used to test the model once it learned the train set. \n",
        "tail examples may lead to bad classify.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVA8j4X1dDWY",
        "colab_type": "text"
      },
      "source": [
        "Here we look at the normal distribution of the features based on the min-max values per class.\n",
        "\n",
        "Analizar la distribuci√≥n de valores por feature para las clases 0 y 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1rmszwHc0zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = train_data.loc[train_data['target'] == 0]\n",
        "t1 = train_data.loc[train_data['target'] == 1]\n",
        "\n",
        "def plot_max_min_colum(dtf1,dtf2,label1,label2,features):\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.title(\"Distribution of min values per column in the train set\")\n",
        "    sns.distplot(dtf1[features].min(axis=0),color=\"orange\", kde=True,bins=120, label='target = '+str(label1))\n",
        "    sns.distplot(dtf2[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = '+str(label2))\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.title(\"Distribution of max values per column in the train set\")\n",
        "    sns.distplot(dtf1[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = '+str(label1))\n",
        "    sns.distplot(dtf2[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = '+str(label2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_max_min_colum(t0,t1,'1','0',features_tot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjegxjWweNz5",
        "colab_type": "text"
      },
      "source": [
        "**Looking at the graph above, it is clear that normalization is necessary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7qM-QwecFi",
        "colab_type": "text"
      },
      "source": [
        "Droppped repeated examples, so the model will not be baised. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hoBkj0EePQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=train_data.drop_duplicates(keep='first')\n",
        "print(train_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coDDqbfBe0UM",
        "colab_type": "text"
      },
      "source": [
        "**The data has no repeated values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7eSkptTZhee",
        "colab_type": "text"
      },
      "source": [
        "**Finally, the data for test and train displeyd a similar distribution for almost all the features, it was not necessary to delete features. However this idea may improve time running. Also after balance the data normalization will be  perform.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpyC0Zzglf8D",
        "colab_type": "text"
      },
      "source": [
        "#Featuring engineering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIJ1cndjrLSe",
        "colab_type": "text"
      },
      "source": [
        "Once the process of analyze the data was performed, featuring engineering will be apply in order to capture important info of both classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hroHhAbJs6gC",
        "colab_type": "text"
      },
      "source": [
        "First, we split the data train into X and labels Y. Also, the train data will be split into cross validation and train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sUJTre_rKmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_data.iloc[:,2:]\n",
        "Y=train_data['target']\n",
        "x_test=test_data.iloc[:,1:]\n",
        "print(\"x_train\")\n",
        "print(X.head())\n",
        "print(\"Shape new X train:\", X.shape)\n",
        "print(\"y_train\")\n",
        "print(Y.head())\n",
        "print(Y.shape)\n",
        "print(\"test_data\")\n",
        "print(x_test.head())\n",
        "print(\"Shape new X test:\",x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnPmuuvktHTl",
        "colab_type": "text"
      },
      "source": [
        "Now we add to the data the statistical values based on the examples (rows), such as mean, std, max, min, med. This features will allow the model to learn new info in order to predict for the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tIYaUKAlXOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = train_data.columns.values[2:202]\n",
        "for df in [X, x_test]:\n",
        "    df['sum'] = df[idx].sum(axis=1)  \n",
        "    df['min'] = df[idx].min(axis=1)\n",
        "    df['max'] = df[idx].max(axis=1)\n",
        "    df['mean'] = df[idx].mean(axis=1)\n",
        "    df['std'] = df[idx].std(axis=1)\n",
        "    df['med'] = df[idx].median(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsU2-BsolEvQ",
        "colab_type": "text"
      },
      "source": [
        "Now, we normalize the data per column, with std 1 and mean 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw-o7fNKuBOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "    \"\"\"\n",
        "    Normalize the data\n",
        "    input dataframe\n",
        "    output dataframe normalized\n",
        "    \"\"\"\n",
        "    #Following the formula \n",
        "    # z =  x - U / g \n",
        "    stats=x.describe()\n",
        "    stats=stats.transpose()\n",
        "    return (x - stats['mean'])/stats['std']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyUsIyg7lURW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=norm(train_data)\n",
        "test_data=norm(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDVaCeDuVt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=norm(X)\n",
        "x_test=norm(x_test)\n",
        "#X.to_pickle(\"x_train_normalized.pkl\")\n",
        "#Y.to_pickle(\"y_train_normalized.pkl\")\n",
        "#x_test.to_pickle(\"x_test_normalized.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFXloILmvP6w",
        "colab_type": "text"
      },
      "source": [
        "#Modelo DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtvjLJnvULV",
        "colab_type": "text"
      },
      "source": [
        "Una vez al tener el dataset necesario se genera el modelo con el cual se clasificar√° el usuario bajo la etiqueta de 0(realiza la transaacci√≥n) y 1(no realiz√° la transacci√≥n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJ75n3ovu0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train=pd.read_pickle('x_train_normalized.pkl')\n",
        "#y_train=pd.read_pickle('y_train_normalized.pkl')\n",
        "#x_predic=pd.read_pickle('x_test_normalized.pkl')\n",
        "tst_size=0.3\n",
        "x_train=X.copy()\n",
        "y_train=Y.copy()\n",
        "x_predic=x_test.copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDlxkJP0KLr",
        "colab_type": "text"
      },
      "source": [
        "Se dividio en dataset en entrenamiento y prueba. Posteriormente se divide el entrenamiento en una fracci√≥n de 0.2 para validaci√≥n cruzada.\n",
        "Quedando finalmente:\n",
        "\n",
        "\n",
        "> entrenamiento pasa de 32958 a 26366\n",
        "\n",
        "> validaci√≥n cruzada  6592\n",
        "\n",
        "> prueba 14125\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBxjqlov0JiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test train split\n",
        "\n",
        "X_train, X_test, Y_train, y_test = train_test_split(x_train, y_train, test_size=tst_size, random_state=6666)\n",
        "print(\"X_train: \", X_train.shape)\n",
        "#print((X_train.head()))\n",
        "print(\"X_test: \" ,X_test.shape)\n",
        "#print(X_test.head())\n",
        "print(\"Y_train: \",Y_train.shape)\n",
        "#print((Y_train.head()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnJmHc-1EgN",
        "colab_type": "text"
      },
      "source": [
        "Se crean tres modelos diferentes, variando la funcion de optimizaci√≥n, el numero de capas ocultas y las funciones de activaci√≥n seg√∫n criterio de dise√±o. Adicionalmente para el primer modelo se utilizan regulaziraci√≥n por L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPZfCwm61AcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buil_model(option=1,layer_size=100,hidden_units=3):\n",
        "\t\"\"\"\n",
        "\tcreate a DNN model\n",
        "\tinput option to select different models, # hidden neurons,  # hidenn units\n",
        "\touput DNN model \n",
        "\t\"\"\"\n",
        "\tif option == 1:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(200,input_shape=(X_train.shape[1],),kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.3))\t\t\n",
        "\t\t\n",
        "\t\tmodel.add(layers.Dense(200,kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.2))\n",
        "\t\t\n",
        "\n",
        "\t\tmodel.add(layers.Dense(150,kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.2))\n",
        "\n",
        "\t\tmodel.add(layers.Dense(50,kernel_regularizer=keras.regularizers.l2(0.005)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.1))\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tmodel.add(layers.Dense(1,activation='sigmoid'))\n",
        "\t\tadam=keras.optimizers.Adam(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc',f1_m,precision_m,recall_m])\n",
        "\t\treturn model\n",
        "\tif option == 2:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(layer_size,input_shape=(x_train.shape[1],)))\n",
        "\t\tmodel.add(layers.Activation('sigmoid'))\n",
        "\t\tmodel.add(layers.Dropout(0.2))\n",
        "\t\tfor i in range(hidden_units):\n",
        "\t\t\tmodel.add(layers.Dense(layer_size))\n",
        "\t\t\tmodel.add(layers.Activation('sigmoid'))\n",
        "\t\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\tmodel.add(layers.Dense(1,activation='relu'))\n",
        "\t\tadam=keras.optimizers.RMSprop(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc',f1_m,precision_m,recall_m])\n",
        "\t\treturn model\n",
        "\n",
        "\tif option == 3:\n",
        "\t\tmodel = keras.Sequential()\n",
        "\t\tmodel.add(layers.Dense(layer_size,input_shape=(x_train.shape[1],)))\n",
        "\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\tmodel.add(layers.Dropout(0.2))\n",
        "\t\tfor i in range(hidden_units):\n",
        "\t\t\tmodel.add(layers.Dense(layer_size))\n",
        "\t\t\tmodel.add(layers.Activation('relu'))\n",
        "\t\t\tmodel.add(layers.Dropout(0.3))\n",
        "\t\tmodel.add(layers.Dense(1,activation='sigmoid'))\n",
        "\t\tadam=keras.optimizers.SGD(lr=0.001)\n",
        "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc',f1_m,precision_m,recall_m])\n",
        "\t\treturn model\n",
        "\telse:\n",
        "\t\traise ValueError('func called with bad args')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOfkfs6m1VbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1=buil_model(1)\n",
        "model2=buil_model(2)\n",
        "model3=buil_model(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9pZ03I51o6D",
        "colab_type": "text"
      },
      "source": [
        "Se utiliza Earlystoping con el objeto de detener el entrenamiento si el criterio de validaci√≥n cruzada no muestra mejor√≠a pasadas 15 epocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84zsL5xU1oJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qr5kev2BVq",
        "colab_type": "text"
      },
      "source": [
        "Se entrenan los modelos utilizando batch de tama√±o 32 y por 100 epocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi6JOsrD17RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model1.summary())\n",
        "history1=model1.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2, \n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_8taH4a18_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model2.summary())\n",
        "history2=model2.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2,\n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdlx6Ec1_1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model3.summary())\n",
        "history3=model3.fit(\n",
        " \tX_train, Y_train,batch_size=32,verbose=1, epochs=100, validation_split =0.2,\n",
        " \tcallbacks=[early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttI1bKxB2SP6",
        "colab_type": "text"
      },
      "source": [
        "Una vez entrenados los modelos se compara las curvas de aprendizaje "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODJbOXZj2RmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history1,history2,history3):\n",
        "\t\"\"\"\n",
        "\tPlot the acc,los for both validation and train for each model.\n",
        "\t\"\"\"\n",
        "\n",
        "\thist1 = pd.DataFrame(history1.history)\n",
        "\thist1['epoch'] = history1.epoch\n",
        "\thist2 = pd.DataFrame(history2.history)\n",
        "\thist2['epoch'] = history2.epoch\n",
        "\thist3 = pd.DataFrame(history3.history)\n",
        "\thist3['epoch'] = history3.epoch\n",
        "\n",
        "\n",
        "\t#print(\"\\n\",hist1.head())\n",
        "\t#print(\"\\n\",hist1.tail())\n",
        "\tacc=history1.history['acc']\n",
        "\tval_acc=history1.history['val_acc']\n",
        "\tloss=history1.history['loss']\n",
        "\tval_loss=history1.history['val_loss']\n",
        "\n",
        "\tacc2=history2.history['acc']\n",
        "\tval_acc2=history2.history['val_acc']\n",
        "\tloss2=history2.history['loss']\n",
        "\tval_loss2=history2.history['val_loss']\n",
        "\n",
        "\tacc3=history3.history['acc']\n",
        "\tval_acc3=history3.history['val_acc']\n",
        "\tloss3=history3.history['loss']\n",
        "\tval_loss3=history3.history['val_loss']\n",
        "\n",
        "\n",
        "\n",
        "\tplt.figure(figsize=(8,8))\n",
        "\tplt.subplot(3,2,1)\n",
        "\tplt.plot(hist1['epoch'],acc,label='Training Accuracy')\n",
        "\tplt.plot(hist1['epoch'],val_acc,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model1')\n",
        "\n",
        "\tplt.subplot(3,2,2)\n",
        "\tplt.plot(hist1['epoch'],loss,label='Training Loss')\n",
        "\tplt.plot(hist1['epoch'],val_loss,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model1')\n",
        "\n",
        "\tplt.subplot(3,2,3)\n",
        "\tplt.plot(hist2['epoch'],acc2,label='Training Accuracy')\n",
        "\tplt.plot(hist2['epoch'],val_acc2,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model2')\n",
        "\n",
        "\tplt.subplot(3,2,4)\n",
        "\tplt.plot(hist2['epoch'],loss2,label='Training Loss')\n",
        "\tplt.plot(hist2['epoch'],val_loss2,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model2')\n",
        "\n",
        "\tplt.subplot(3,2,5)\n",
        "\tplt.plot(hist3['epoch'],acc3,label='Training Accuracy')\n",
        "\tplt.plot(hist3['epoch'],val_acc3,label='Validation Accuracy')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Accuracy model3')\n",
        "\n",
        "\tplt.subplot(3,2,6)\n",
        "\tplt.plot(hist3['epoch'],loss3,label='Training Loss')\n",
        "\tplt.plot(hist3['epoch'],val_loss3,label='Validation Loss')\n",
        "\tplt.legend()\n",
        "\tplt.title('Training and Validation Loss model3')\n",
        "\n",
        "\tplt.show()\n",
        "\n",
        "plot_history(history1,history2,history3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "METrcgcS7mv0",
        "colab_type": "text"
      },
      "source": [
        "**A partir de la grafica de curva de aprendizaje se puede observar cuales  modelos logran generalizar en mejor medida los datos. Se observa que en promedio para la epoca 12 los modelos empiezan a generar overfitting. En especial el modelo 2. En general los modelos logran una precisi√≥n del 78+-2 en entrenamiento y 75+-2 en validaci√≥n. Lo cual es un valor medianemente bueno. Para mejorar el resultado existen algunas posibilidades como: generar mayor cantidad de datos para la clase 1 y as√≠ entrenar con todo el dataser inicial.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPTNTosu24Hy",
        "colab_type": "text"
      },
      "source": [
        "Se evaluan los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b57d73o42ak1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model evualte \n",
        "test_lost1,test_accuracy1 = model1.evaluate(X_test,y_test,verbose=1)\n",
        "test_lost2,test_accuracy2 = model2.evaluate(X_test,y_test,verbose=1)\n",
        "test_lost3,test_accuracy3 = model3.evaluate(X_test,y_test,verbose=1)\n",
        "print('model 1(Adam) test loss: {} accuracy test: {} '.format(test_lost1,test_accuracy1))\n",
        "print('model 2(RMSprop) test loss: {} accuracy test: {} '.format(test_lost2,test_accuracy2))\n",
        "print('model 3(SGD) test loss: {} accuracy test: {} '.format(test_lost3,test_accuracy3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdy7ZaFv36oe",
        "colab_type": "text"
      },
      "source": [
        "Se predice usando los modelos entrenados y datos sin etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNFAdrRx31l_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_pred_nn = model1.predict(x_predic)[:,0]\n",
        "target_pred_nn2 = model2.predict(x_predic)[:,0]\n",
        "target_pred_nn3 = model3.predict(x_predic)[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzRL5b-E3vsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13, 9))\n",
        "sns.distplot(target_pred_nn,label='NN model 1(Adam) Target')\n",
        "sns.distplot(target_pred_nn2,label='NN model 2(RMSprop) Target')\n",
        "sns.distplot(target_pred_nn3,label='NN model 3(SGD) Target')\n",
        "plt.title('Test set target predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}